#!/usr/bin/python
# -*- coding: utf-8 -*-
__author__ = 'ZS Associates'

"""
Doc_Type         : Business
Description      : The purpose of this code is to execute unified layer ingestion for 
                   f_sales_account_direct_sales  table.
Tech_Description : N/A.
Pre_requisites   : Parquet data in source - current layer should be available.
Inputs           : Domain, Subdomain, Filename, Vendor, Data Source, Batch ID, File ID, Environment
Outputs          : Parquet data in S3 Unified Layer for f_sales_account_direct_sales table.
                   Error message (in case of error)
Example          : Parameters replaced in this code
                    {
                     domain = f_sales
                     subdomain = account
                     data_source = gcps_daily_direct_sales
                     vendor = gcps
                     file_name = file name fetched by process
                     pt_batch_id = auto generated by the process
                     pt_file_id = auto generated by the process
                     environment = dev/tst/prd
                    }
Config_File      : PlatformEnvParams.json, EnvCredentials.json
External_Link    : N/A
"""


# Import python packages/libraries
from pyspark.sql import functions as F
from pyspark.sql.functions import *
from pyspark.sql.types import *
import datetime


# Initialize file_name
file_name="$$file_name$$"
file_split = file_name.split("_")
timestamp = file_split[3]
year = int(timestamp[0:4])
month = int(timestamp[5:7])
day = int(timestamp[8:10])
date = datetime.date(year, month, day)
week = date.isocalendar()[1]

# Load data from source-current layer into the data frame with filters
f_sales_account_direct_sales_unified_df = spark.table("$$environment$$_source_gcps.gcps_daily_direct_sales_gcps_direct_sales")\
    .where((col("pt_batch_id")=='$$pt_batch_id$$') & (col('pt_file_id')=='$$pt_file_id$$'))


# Generate CDL columns and assign their value
f_sales_account_direct_sales_unified_df = f_sales_account_direct_sales_unified_df.withColumn('cdl_domain', lit("$$domain$$"))\
    .withColumn('cdl_sub_domain', lit("$$sub_domain$$")).withColumn('cdl_data_source', lit("$$data_source_name$$"))\
    .withColumn('cdl_file_name', lit("$$file_name$$")).withColumn('cdl_batch_id', lit('$$pt_batch_id$$'))\
    .withColumn('cdl_row_id', expr("row_number() over (partition by 1 order by 1)"))\
    .withColumn('cdl_year', lit(year))\
    .withColumn('cdl_semester', when(lit(month)<=6, 1 ).otherwise(2))\
    .withColumn('cdl_month', lit(month))\
    .withColumn('cdl_quarter', quarter(lit(date)))\
    .withColumn('cdl_week', lit(week))\
    .withColumn("cdl_day",lit(day))


# Steps to Implement data value standardization
f_sales_account_direct_sales_unified_df = f_sales_account_direct_sales_unified_df\
.withColumnRenamed("original_inv_line_number","original_invoice_line_number")\
.withColumnRenamed("payment_term_descr","payment_term_description")\


# Casting cdl generated columns to correct data types
f_sales_account_direct_sales_unified_df = f_sales_account_direct_sales_unified_df \
    .withColumn('cdl_domain', f_sales_account_direct_sales_unified_df["cdl_domain"].cast("string")) \
    .withColumn('cdl_sub_domain', f_sales_account_direct_sales_unified_df["cdl_sub_domain"].cast("string")) \
    .withColumn('cdl_data_source', f_sales_account_direct_sales_unified_df["cdl_data_source"].cast("string")) \
    .withColumn('cdl_file_name', f_sales_account_direct_sales_unified_df["cdl_file_name"].cast("string")) \
    .withColumn('cdl_batch_id', f_sales_account_direct_sales_unified_df["cdl_batch_id"].cast("string")) \
    .withColumn('cdl_row_id', f_sales_account_direct_sales_unified_df["cdl_row_id"].cast("int")) \
    .withColumn('cdl_year', f_sales_account_direct_sales_unified_df["cdl_year"].cast("int")) \
    .withColumn("cdl_semester", f_sales_account_direct_sales_unified_df["cdl_semester"].cast("int")) \
    .withColumn('cdl_quarter', f_sales_account_direct_sales_unified_df["cdl_quarter"].cast("int")) \
    .withColumn('cdl_month', f_sales_account_direct_sales_unified_df["cdl_month"].cast("int")) \
    .withColumn("cdl_week", f_sales_account_direct_sales_unified_df["cdl_week"].cast("int")) \
    .withColumn("cdl_day", f_sales_account_direct_sales_unified_df["cdl_day"].cast("int")) \
    .withColumn("pt_batch_id", f_sales_account_direct_sales_unified_df["pt_batch_id"].cast("string")) \
    .withColumn("pt_file_id", f_sales_account_direct_sales_unified_df["pt_file_id"].cast("int"))\
    .withColumn('cdl_record_load_timestamp', expr("CURRENT_TIMESTAMP()"))\
    .withColumn("cdl_source_system_batch_id", lit(None).cast(StringType()))

# Reorder columns based on the table structure
f_sales_account_direct_sales_unified_df = f_sales_account_direct_sales_unified_df.select("cdl_domain","cdl_sub_domain","cdl_data_source","cdl_file_name","cdl_batch_id","cdl_row_id","cdl_year","cdl_semester","cdl_quarter","cdl_month","cdl_week","cdl_day","invoice_number","line_number","original_invoice_line_number","sold_to_id","bill_to_id","ship_to_id","payment_term","payment_term_description","material_number","item_id","invoice_date","order_reference","invoice_amount","invoice_quantity","uom","extended_invoice_amount","contract_id","contract_base_price","order_type","reason_code","sales_order_number","sales_order_line_number","sales_order_date","sales_order_price_date","bill_type","wac","line_reason_code","cdl_source_system_batch_id","cdl_record_load_timestamp","pt_batch_id","pt_file_id")



# Write back parquet files into unified table
f_sales_account_direct_sales_unified_df.write.mode("overwrite").parquet('s3a://$$cdl_s3_bucket_name$$/cdl/unified/f_sales/account/f_sales_account_direct_sales/pt_batch_id=$$pt_batch_id$$/pt_file_id=$$pt_file_id$$/')

# Metadata refresh command
sqlContext.sql("alter table $$environment$$_unified_f_sales.f_sales_account_direct_sales"
    " add if not exists partition (pt_batch_id='$$pt_batch_id$$',pt_file_id='$$pt_file_id$$')")